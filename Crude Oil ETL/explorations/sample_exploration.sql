-- Databricks notebook source
-- MAGIC %md
-- MAGIC ### Example Exploratory Notebook
-- MAGIC
-- MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
-- MAGIC
-- MAGIC **Note**: This notebook is not executed as part of the pipeline.

-- COMMAND ----------

-- MAGIC %python
-- MAGIC import sys
-- MAGIC
-- MAGIC sys.path.append("/Workspace/Users/wajdi.bounouara@databricks.com/Crude Oil ETL")

-- COMMAND ----------

-- !!! Before performing any data analysis, make sure to run the pipeline to materialize the sample datasets. The tables referenced in this notebook depend on that step.

USE CATALOG `wajdi_bounouara`;
USE SCHEMA `parex_resources`;

SELECT * from sample_aggregation_crude_oil_etl;

-- COMMAND ----------

SELECT
    value::string:reading_id::string as reading_id,
    value::string:sensor_id::string as sensor_id,
    value::string:unit_id::string as unit_id,
    value::string:refinery_id::string as refinery_id,
    value::string:reading_timestamp::string as reading_timestamp,
    value::string:reading_value::double as reading_value,
    value::string:reading_quality::string as reading_quality,
    value::string:is_interpolated:boolean as is_interpolated,
    value::string:event_time::string as event_time,
    value::string:processing_time:string as processing_time 

-- FROM STREAM read_kafka(
--     bootstrapServers => "parex-demo.servicebus.windows.net:9093",
--     subscribe => "sensor-readings"
-- )
FROM STREAM read_kafka(
    bootstrapServers => "parex-demo.servicebus.windows.net:9093",
    subscribe => "sensor-readings",
    `kafka.security.protocol` => 'SASL_SSL',
    `kafka.sasl.mechanism` => 'PLAIN',
    `kafka.sasl.jaas.config` => 'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username="\\$ConnectionString" password="Endpoint=sb://parex-demo.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=gSro4wk2U7a8TzBuZgAVXhIaPPloFmt6E+AEhOJP0wo=";',
    `kafka.request.timeout.ms` => '60000',
    `kafka.session.timeout.ms` => '30000',
    startingOffsets => 'latest'
)

-- COMMAND ----------

SELECT 
    sr.reading_id,
    sr.sensor_id,
    sr.unit_id,
    sr.refinery_id,
    sr.reading_timestamp,
    sr.reading_value,
    sr.reading_quality,
    s.sensor_type,
    s.measurement_unit,
    s.warning_low,
    s.warning_high,
    s.critical_low,
    s.critical_high,
    -- Anomaly detection
    CASE 
        WHEN sr.reading_value < s.critical_low OR sr.reading_value > s.critical_high THEN 'CRITICAL'
        WHEN sr.reading_value < s.warning_low OR sr.reading_value > s.warning_high THEN 'WARNING'
        ELSE 'NORMAL'
    END AS alert_status,
    -- Data quality flags
    (sr.reading_value < s.min_value OR sr.reading_value > s.max_value) AS is_out_of_range,
    sr.event_time,
    sr.processing_time,
    current_timestamp() AS silver_processed_at
FROM bronze_fact_sensor_readings sr
LEFT JOIN bronze_dim_sensors s ON sr.sensor_id = s.sensor_id
WHERE s.is_active = true

UNION ALL

SELECT 
    srt.reading_id,
    srt.sensor_id,
    srt.unit_id,
    srt.refinery_id,
    srt.reading_timestamp,
    srt.reading_value,
    srt.reading_quality,
    s.sensor_type,
    s.measurement_unit,
    s.warning_low,
    s.warning_high,
    s.critical_low,
    s.critical_high,
    -- Anomaly detection
    CASE 
        WHEN srt.reading_value < s.critical_low OR srt.reading_value > s.critical_high THEN 'CRITICAL'
        WHEN srt.reading_value < s.warning_low OR srt.reading_value > s.warning_high THEN 'WARNING'
        ELSE 'NORMAL'
    END AS alert_status,
    -- Data quality flags
    (srt.reading_value < s.min_value OR srt.reading_value > s.max_value) AS is_out_of_range,
    srt.event_time,
    srt.processing_time,
    current_timestamp() AS silver_processed_at
FROM bronze_fact_sensor_readings_stream srt
LEFT JOIN bronze_dim_sensors s ON srt.sensor_id = s.sensor_id
WHERE s.is_active = true