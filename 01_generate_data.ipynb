{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a37b088-b3d8-4ffc-9856-280af49e2c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install dbldatagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8499c5b8-5694-486f-87a6-6c6cffb40220",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Data Generation Parameters for Crude Oil Pip ..."
    }
   },
   "outputs": [],
   "source": [
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "NUM_REFINERIES = 500\n",
    "SENSORS_PER_REFINERY = 1000\n",
    "DAYS_OF_HISTORY = 365\n",
    "OUTPUT_PATH = \"/mnt/data/crude_oil_pipeline\"\n",
    "CATALOG=\"wajdi_bounouara\"\n",
    "SCHEMA=\"parex_resources\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa2527d8-1e1e-4fa6-9301-f071f00cd61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af8dbef-72a5-403f-bf5b-507435a638fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_refineries ---\n",
    "def generate_refineries(spark, num_refineries=500):\n",
    "    cities = [\n",
    "        (\"Houston\", \"TX\", \"USA\", 29.76, -95.37),\n",
    "        (\"Beaumont\", \"TX\", \"USA\", 30.08, -94.10),\n",
    "        (\"Lake Charles\", \"LA\", \"USA\", 30.21, -93.22),\n",
    "        (\"Baton Rouge\", \"LA\", \"USA\", 30.45, -91.15),\n",
    "        (\"New Orleans\", \"LA\", \"USA\", 29.95, -90.07),\n",
    "        (\"Port Arthur\", \"TX\", \"USA\", 29.90, -93.93),\n",
    "        (\"Corpus Christi\", \"TX\", \"USA\", 27.80, -97.40),\n",
    "        (\"Philadelphia\", \"PA\", \"USA\", 39.95, -75.17),\n",
    "        (\"Los Angeles\", \"CA\", \"USA\", 34.05, -118.24),\n",
    "        (\"San Francisco\", \"CA\", \"USA\", 37.77, -122.42),\n",
    "        (\"Chicago\", \"IL\", \"USA\", 41.88, -87.63),\n",
    "        (\"Detroit\", \"MI\", \"USA\", 42.33, -83.05),\n",
    "        (\"Rotterdam\", \"ZH\", \"NLD\", 51.92, 4.48),\n",
    "        (\"Singapore\", \"SG\", \"SGP\", 1.35, 103.82),\n",
    "        (\"Jamnagar\", \"GJ\", \"IND\", 22.47, 70.07),\n",
    "    ]\n",
    "    \n",
    "    operators = [\"PetroCorp\", \"GlobalRefining\", \"CoastalEnergy\", \"TransOil\", \n",
    "                 \"PacificPetroleum\", \"AtlanticFuels\", \"MidwestRefinery\", \"SunsetOil\"]\n",
    "    \n",
    "    refinery_spec = (\n",
    "        dg.DataGenerator(spark, name=\"refineries\", rows=num_refineries, partitions=4)\n",
    "        .withColumn(\"refinery_id\", \"string\", expr=\"concat('REF-', lpad(cast(id as string), 3, '0'))\")\n",
    "        .withColumn(\"city_idx\", \"int\", minValue=0, maxValue=len(cities)-1, random=True)\n",
    "        .withColumn(\"refinery_name\", \"string\", \n",
    "                   expr=f\"concat(element_at(array{tuple([c[0] for c in cities])}, city_idx+1), ' Refinery ', chr(65 + (id % 26)))\")\n",
    "        .withColumn(\"location_city\", \"string\", \n",
    "                   expr=f\"element_at(array{tuple([c[0] for c in cities])}, city_idx+1)\")\n",
    "        .withColumn(\"location_state\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([c[1] for c in cities])}, city_idx+1)\")\n",
    "        .withColumn(\"location_country\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([c[2] for c in cities])}, city_idx+1)\")\n",
    "        .withColumn(\"latitude\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([c[3] for c in cities])}, city_idx+1) + (rand() - 0.5) * 0.5\")\n",
    "        .withColumn(\"longitude\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([c[4] for c in cities])}, city_idx+1) + (rand() - 0.5) * 0.5\")\n",
    "        .withColumn(\"capacity_bpd\", \"int\", minValue=50000, maxValue=700000, step=10000, random=True)\n",
    "        .withColumn(\"complexity_index\", \"double\", minValue=5.0, maxValue=15.0, random=True)\n",
    "        .withColumn(\"commissioned_date\", \"date\", \n",
    "                   begin=\"1960-01-01\", end=\"2020-12-31\", random=True)\n",
    "        .withColumn(\"operator_name\", \"string\", values=operators, random=True)\n",
    "        .withColumn(\"is_active\", \"boolean\", expr=\"rand() > 0.05\")  # 95% active\n",
    "        .withColumn(\"last_turnaround_date\", \"date\",\n",
    "                   begin=\"2020-01-01\", end=\"2024-06-30\", random=True)\n",
    "    )\n",
    "    \n",
    "    df = refinery_spec.build()\n",
    "    return df.drop(\"city_idx\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b078e3-98c3-453b-a575-5ab2683c1b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_processing_units ---\n",
    "def generate_processing_units(spark, refineries_df, avg_units_per_refinery=10):\n",
    "    unit_types = [\n",
    "        (\"Crude Distillation\", \"Atmospheric\", 0.95),\n",
    "        (\"Crude Distillation\", \"Vacuum\", 0.93),\n",
    "        (\"Fluid Catalytic Cracker\", \"Resid FCC\", 0.90),\n",
    "        (\"Hydrocracker\", \"Mild\", 0.92),\n",
    "        (\"Hydrocracker\", \"Full Conversion\", 0.88),\n",
    "        (\"Reformer\", \"Continuous\", 0.91),\n",
    "        (\"Reformer\", \"Semi-Regenerative\", 0.89),\n",
    "        (\"Alkylation\", \"HF Process\", 0.94),\n",
    "        (\"Alkylation\", \"Sulfuric Acid\", 0.93),\n",
    "        (\"Coker\", \"Delayed\", 0.87),\n",
    "        (\"Coker\", \"Fluid\", 0.85),\n",
    "        (\"Hydrotreater\", \"Naphtha\", 0.96),\n",
    "        (\"Hydrotreater\", \"Diesel\", 0.95),\n",
    "        (\"Sulfur Recovery\", \"Claus\", 0.98),\n",
    "    ]\n",
    "    \n",
    "    manufacturers = [\"Honeywell UOP\", \"Axens\", \"Shell Global\", \"ExxonMobil\", \n",
    "                    \"Chevron Lummus\", \"KBR\", \"CB&I\", \"Technip\"]\n",
    "    \n",
    "    refinery_ids = [row.refinery_id for row in refineries_df.select(\"refinery_id\").collect()]\n",
    "    total_units = len(refinery_ids) * avg_units_per_refinery\n",
    "    \n",
    "    unit_spec = (\n",
    "        dg.DataGenerator(spark, name=\"processing_units\", rows=total_units, partitions=8)\n",
    "        .withColumn(\"unit_id\", \"string\", expr=\"concat('UNIT-', lpad(cast(id as string), 5, '0'))\")\n",
    "        .withColumn(\"refinery_idx\", \"int\", minValue=0, maxValue=len(refinery_ids)-1, random=True)\n",
    "        .withColumn(\"refinery_id\", \"string\", \n",
    "                   expr=f\"element_at(array{tuple(refinery_ids)}, refinery_idx+1)\")\n",
    "        .withColumn(\"unit_type_idx\", \"int\", minValue=0, maxValue=len(unit_types)-1, random=True)\n",
    "        .withColumn(\"unit_type\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([u[0] for u in unit_types])}, unit_type_idx+1)\")\n",
    "        .withColumn(\"unit_subtype\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([u[1] for u in unit_types])}, unit_type_idx+1)\")\n",
    "        .withColumn(\"design_efficiency\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([u[2] for u in unit_types])}, unit_type_idx+1)\")\n",
    "        .withColumn(\"unit_name\", \"string\",\n",
    "                   expr=\"concat(substring(unit_type, 1, 3), '-', (id % 10) + 1)\")\n",
    "        .withColumn(\"capacity_bpd\", \"int\", minValue=10000, maxValue=200000, step=5000, random=True)\n",
    "        .withColumn(\"installation_date\", \"date\", begin=\"1980-01-01\", end=\"2023-12-31\", random=True)\n",
    "        .withColumn(\"last_maintenance_date\", \"date\", begin=\"2023-01-01\", end=\"2025-06-30\", random=True)\n",
    "        .withColumn(\"maintenance_interval_days\", \"int\", values=[90, 180, 365], random=True)\n",
    "        .withColumn(\"manufacturer\", \"string\", values=manufacturers, random=True)\n",
    "        .withColumn(\"model_number\", \"string\", \n",
    "                   expr=\"concat(substring(manufacturer, 1, 2), '-', cast(floor(rand()*9000+1000) as int))\")\n",
    "        .withColumn(\"is_active\", \"boolean\", expr=\"rand() > 0.03\")\n",
    "    )\n",
    "    \n",
    "    df = unit_spec.build()\n",
    "    return df.drop(\"refinery_idx\", \"unit_type_idx\", \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0367b686-2046-46b3-b8f0-043af358420a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_sensors ---\n",
    "def generate_sensors(spark, units_df, avg_sensors_per_unit=100):\n",
    "    sensor_types = [\n",
    "        (\"Temperature\", \"Fahrenheit\", 0, 1200, 200, 900, 100, 1000),\n",
    "        (\"Temperature\", \"Celsius\", -50, 600, 50, 500, 0, 550),\n",
    "        (\"Pressure\", \"PSI\", 0, 5000, 50, 3000, 10, 4000),\n",
    "        (\"Pressure\", \"Bar\", 0, 350, 5, 200, 1, 280),\n",
    "        (\"Flow Rate\", \"BPH\", 0, 50000, 1000, 40000, 500, 45000),\n",
    "        (\"Flow Rate\", \"GPM\", 0, 100000, 2000, 80000, 1000, 90000),\n",
    "        (\"Level\", \"Percent\", 0, 100, 10, 95, 5, 98),\n",
    "        (\"Level\", \"Feet\", 0, 50, 5, 45, 2, 48),\n",
    "        (\"Vibration\", \"mm/s\", 0, 50, 0, 20, 0, 35),\n",
    "        (\"Vibration\", \"mils\", 0, 10, 0, 5, 0, 8),\n",
    "        (\"pH\", \"pH\", 0, 14, 4, 10, 2, 12),\n",
    "        (\"Density\", \"API\", 0, 80, 20, 50, 10, 60),\n",
    "        (\"Viscosity\", \"cSt\", 0, 1000, 1, 500, 0.5, 800),\n",
    "    ]\n",
    "    \n",
    "    manufacturers = [\"Emerson\", \"Honeywell\", \"Siemens\", \"ABB\", \"Yokogawa\", \"Endress+Hauser\"]\n",
    "    \n",
    "    unit_ids = [row.unit_id for row in units_df.select(\"unit_id\").collect()]\n",
    "    unit_refinery_map = {row.unit_id: row.refinery_id \n",
    "                        for row in units_df.select(\"unit_id\", \"refinery_id\").collect()}\n",
    "    \n",
    "    total_sensors = len(unit_ids) * avg_sensors_per_unit\n",
    "    \n",
    "    sensor_spec = (\n",
    "        dg.DataGenerator(spark, name=\"sensors\", rows=total_sensors, partitions=16)\n",
    "        .withColumn(\"sensor_id\", \"string\", expr=\"concat('SENS-', lpad(cast(id as string), 6, '0'))\")\n",
    "        .withColumn(\"unit_idx\", \"int\", minValue=0, maxValue=len(unit_ids)-1, random=True)\n",
    "        .withColumn(\"unit_id\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple(unit_ids)}, unit_idx+1)\")\n",
    "        .withColumn(\"sensor_type_idx\", \"int\", minValue=0, maxValue=len(sensor_types)-1, random=True)\n",
    "        .withColumn(\"sensor_type\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([s[0] for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"measurement_unit\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple([s[1] for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"min_value\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[2]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"max_value\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[3]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"warning_low\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[4]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"warning_high\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[5]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"critical_low\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[6]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"critical_high\", \"double\",\n",
    "                   expr=f\"element_at(array{tuple([float(s[7]) for s in sensor_types])}, sensor_type_idx+1)\")\n",
    "        .withColumn(\"sensor_name\", \"string\",\n",
    "                   expr=\"concat(substring(sensor_type, 1, 4), '-', lpad(cast((id % 1000) as string), 3, '0'))\")\n",
    "        .withColumn(\"reading_interval_seconds\", \"int\", values=[5, 10, 30, 60], weights=[0.1, 0.5, 0.3, 0.1], random=True)\n",
    "        .withColumn(\"manufacturer\", \"string\", values=manufacturers, random=True)\n",
    "        .withColumn(\"model\", \"string\", expr=\"concat(substring(manufacturer, 1, 3), '-', cast(floor(rand()*900+100) as int))\")\n",
    "        .withColumn(\"installation_date\", \"date\", begin=\"2015-01-01\", end=\"2025-01-01\", random=True)\n",
    "        .withColumn(\"calibration_date\", \"date\", begin=\"2023-06-01\", end=\"2025-06-30\", random=True)\n",
    "        .withColumn(\"is_active\", \"boolean\", expr=\"rand() > 0.02\")\n",
    "    )\n",
    "    \n",
    "    df = sensor_spec.build()\n",
    "    \n",
    "    # Add refinery_id via join (more efficient than lookup)\n",
    "    df = df.join(units_df.select(\"unit_id\", \"refinery_id\"), \"unit_id\", \"left\")\n",
    "    \n",
    "    return df.drop(\"unit_idx\", \"sensor_type_idx\", \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac8b03b-4312-44b8-a725-ed4890f7c7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_crude_types ---\n",
    "def generate_crude_types(spark):\n",
    "    crude_data = [\n",
    "        (\"CRUDE-001\", \"West Texas Intermediate\", \"WTI\", \"USA\", \"Permian Basin\", 39.6, 0.24, \"Light\", \"Sweet\", -3.50),\n",
    "        (\"CRUDE-002\", \"Brent\", \"BRENT\", \"UK\", \"North Sea\", 38.3, 0.37, \"Light\", \"Sweet\", 0.00),\n",
    "        (\"CRUDE-003\", \"Dubai\", \"DUBAI\", \"UAE\", \"Persian Gulf\", 31.0, 2.00, \"Medium\", \"Sour\", -2.00),\n",
    "        (\"CRUDE-004\", \"Arab Light\", \"AL\", \"SAU\", \"Ghawar\", 33.0, 1.77, \"Light\", \"Sour\", -1.50),\n",
    "        (\"CRUDE-005\", \"Arab Heavy\", \"AH\", \"SAU\", \"Safaniya\", 27.0, 2.80, \"Heavy\", \"Sour\", -5.00),\n",
    "        (\"CRUDE-006\", \"Bonny Light\", \"BONNY\", \"NGA\", \"Niger Delta\", 33.4, 0.16, \"Light\", \"Sweet\", 2.00),\n",
    "        (\"CRUDE-007\", \"Mars\", \"MARS\", \"USA\", \"Gulf of Mexico\", 31.0, 1.93, \"Medium\", \"Sour\", -4.00),\n",
    "        (\"CRUDE-008\", \"Maya\", \"MAYA\", \"MEX\", \"Campeche\", 22.0, 3.30, \"Heavy\", \"Sour\", -10.00),\n",
    "        (\"CRUDE-009\", \"Urals\", \"URALS\", \"RUS\", \"Western Siberia\", 31.7, 1.35, \"Medium\", \"Sour\", -15.00),\n",
    "        (\"CRUDE-010\", \"Tapis\", \"TAPIS\", \"MYS\", \"Malay Basin\", 44.3, 0.04, \"Light\", \"Sweet\", 5.00),\n",
    "        (\"CRUDE-011\", \"Saharan Blend\", \"SAHARAN\", \"DZA\", \"Hassi Messaoud\", 45.5, 0.09, \"Light\", \"Sweet\", 3.00),\n",
    "        (\"CRUDE-012\", \"Forcados\", \"FORCADOS\", \"NGA\", \"Niger Delta\", 29.6, 0.18, \"Light\", \"Sweet\", 1.50),\n",
    "        (\"CRUDE-013\", \"Ekofisk\", \"EKO\", \"NOR\", \"North Sea\", 43.4, 0.17, \"Light\", \"Sweet\", 1.00),\n",
    "        (\"CRUDE-014\", \"Louisiana Light\", \"LLS\", \"USA\", \"Gulf Coast\", 36.0, 0.40, \"Light\", \"Sweet\", -1.00),\n",
    "        (\"CRUDE-015\", \"Heavy Louisiana Sweet\", \"HLS\", \"USA\", \"Gulf Coast\", 32.5, 0.30, \"Medium\", \"Sweet\", -2.50),\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"crude_type_id\", StringType()),\n",
    "        StructField(\"crude_name\", StringType()),\n",
    "        StructField(\"crude_abbreviation\", StringType()),\n",
    "        StructField(\"origin_country\", StringType()),\n",
    "        StructField(\"origin_region\", StringType()),\n",
    "        StructField(\"api_gravity_typical\", DoubleType()),\n",
    "        StructField(\"sulfur_content_typical\", DoubleType()),\n",
    "        StructField(\"classification\", StringType()),\n",
    "        StructField(\"sweetness\", StringType()),\n",
    "        StructField(\"benchmark_price_differential\", DoubleType()),\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(crude_data, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35ef3fa-4275-4eb5-82ff-6d71cc864cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_products ---\n",
    "def generate_products(spark):\n",
    "    product_data = [\n",
    "        (\"PROD-001\", \"Regular Gasoline\", \"Gasoline\", \"87 Octane\", 720, 775, 10, 0.25),\n",
    "        (\"PROD-002\", \"Premium Gasoline\", \"Gasoline\", \"91 Octane\", 720, 770, 10, 0.15),\n",
    "        (\"PROD-003\", \"Super Premium Gasoline\", \"Gasoline\", \"93 Octane\", 715, 765, 10, 0.10),\n",
    "        (\"PROD-004\", \"Ultra Low Sulfur Diesel\", \"Diesel\", \"ULSD\", 820, 860, 15, 0.28),\n",
    "        (\"PROD-005\", \"Heating Oil\", \"Distillate\", \"No. 2\", 830, 870, 500, 0.08),\n",
    "        (\"PROD-006\", \"Jet Fuel A\", \"Jet Fuel\", \"Jet A\", 775, 840, 3000, 0.08),\n",
    "        (\"PROD-007\", \"Kerosene\", \"Distillate\", \"K-1\", 780, 830, 40, 0.03),\n",
    "        (\"PROD-008\", \"Naphtha\", \"Feedstock\", \"Light\", 650, 720, 500, 0.05),\n",
    "        (\"PROD-009\", \"LPG\", \"Gas\", \"Propane/Butane\", 500, 600, 50, 0.04),\n",
    "        (\"PROD-010\", \"Residual Fuel Oil\", \"Residual\", \"No. 6\", 950, 1010, 35000, 0.03),\n",
    "        (\"PROD-011\", \"Asphalt\", \"Residual\", \"Paving Grade\", 1010, 1060, 50000, 0.02),\n",
    "        (\"PROD-012\", \"Petroleum Coke\", \"Solid\", \"Fuel Grade\", 1400, 1600, 80000, 0.04),\n",
    "        (\"PROD-013\", \"Lubricant Base Oil\", \"Lubricant\", \"Group II\", 850, 890, 10, 0.02),\n",
    "        (\"PROD-014\", \"Sulfur\", \"Byproduct\", \"Eleite\", 1800, 2100, 0, 0.015),\n",
    "    ]\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"product_id\", StringType()),\n",
    "        StructField(\"product_name\", StringType()),\n",
    "        StructField(\"product_category\", StringType()),\n",
    "        StructField(\"product_grade\", StringType()),\n",
    "        StructField(\"density_min\", DoubleType()),\n",
    "        StructField(\"density_max\", DoubleType()),\n",
    "        StructField(\"sulfur_max_ppm\", IntegerType()),\n",
    "        StructField(\"typical_yield_pct\", DoubleType()),\n",
    "    ])\n",
    "    \n",
    "    df = spark.createDataFrame(product_data, schema)\n",
    "    return df.withColumn(\"unit_of_measure\", F.lit(\"barrels\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be8963dc-5f7e-4d05-a3ea-9f463a825da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- dim_storage_tanks ---\n",
    "def generate_storage_tanks(spark, refineries_df, avg_tanks_per_refinery=4):\n",
    "    tank_types = [\"Floating Roof\", \"Fixed Roof\", \"Spherical\", \"Horizontal\"]\n",
    "    product_types = [\"Crude Oil\", \"Gasoline\", \"Diesel\", \"Jet Fuel\", \"Residual\"]\n",
    "    \n",
    "    refinery_ids = [row.refinery_id for row in refineries_df.select(\"refinery_id\").collect()]\n",
    "    total_tanks = len(refinery_ids) * avg_tanks_per_refinery\n",
    "    \n",
    "    tank_spec = (\n",
    "        dg.DataGenerator(spark, name=\"storage_tanks\", rows=total_tanks, partitions=4)\n",
    "        .withColumn(\"tank_id\", \"string\", expr=\"concat('TANK-', lpad(cast(id as string), 5, '0'))\")\n",
    "        .withColumn(\"refinery_idx\", \"int\", minValue=0, maxValue=len(refinery_ids)-1, random=True)\n",
    "        .withColumn(\"refinery_id\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple(refinery_ids)}, refinery_idx+1)\")\n",
    "        .withColumn(\"tank_name\", \"string\", expr=\"concat('TK-', lpad(cast((id % 500) + 100 as string), 3, '0'))\")\n",
    "        .withColumn(\"tank_type\", \"string\", values=tank_types, weights=[0.4, 0.3, 0.15, 0.15], random=True)\n",
    "        .withColumn(\"product_type\", \"string\", values=product_types, weights=[0.3, 0.25, 0.2, 0.15, 0.1], random=True)\n",
    "        .withColumn(\"capacity_barrels\", \"int\", values=[100000, 250000, 500000, 750000, 1000000], \n",
    "                   weights=[0.2, 0.3, 0.25, 0.15, 0.1], random=True)\n",
    "        .withColumn(\"min_operating_level\", \"double\", minValue=0.08, maxValue=0.15, random=True)\n",
    "        .withColumn(\"max_operating_level\", \"double\", minValue=0.90, maxValue=0.98, random=True)\n",
    "        .withColumn(\"diameter_feet\", \"double\", minValue=100, maxValue=350, random=True)\n",
    "        .withColumn(\"height_feet\", \"double\", minValue=40, maxValue=65, random=True)\n",
    "        .withColumn(\"material\", \"string\", values=[\"Carbon Steel\", \"Stainless Steel\"], weights=[0.8, 0.2], random=True)\n",
    "        .withColumn(\"has_heating\", \"boolean\", expr=\"rand() > 0.6\")\n",
    "        .withColumn(\"has_mixer\", \"boolean\", expr=\"rand() > 0.5\")\n",
    "        .withColumn(\"last_inspection_date\", \"date\", begin=\"2022-01-01\", end=\"2025-06-30\", random=True)\n",
    "        .withColumn(\"is_active\", \"boolean\", expr=\"rand() > 0.05\")\n",
    "    )\n",
    "    \n",
    "    df = tank_spec.build()\n",
    "    return df.drop(\"refinery_idx\", \"id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe157e3-a324-4095-b9c6-36d83cb28da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683c7d1d-0a5e-4cbb-97d7-3c6dc87b1b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- fact_sensor_readings (STREAMING) ---\n",
    "def generate_sensor_readings_batch(spark, sensors_df, start_date, end_date, sample_rate=0.01):\n",
    "    \"\"\"\n",
    "    Generate historical sensor readings.\n",
    "    sample_rate: fraction of sensors × time intervals to generate (0.01 = 1%)\n",
    "    For full data, use sample_rate=1.0 but expect ~70GB+\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    # Get sensor metadata\n",
    "    sensor_data = sensors_df.select(\n",
    "        \"sensor_id\", \"unit_id\", \"refinery_id\", \"sensor_type\",\n",
    "        \"min_value\", \"max_value\", \"reading_interval_seconds\"\n",
    "    )\n",
    "    \n",
    "    # Calculate time range\n",
    "    days = (end_date - start_date).days\n",
    "    \n",
    "    # Generate readings\n",
    "    readings_spec = (\n",
    "        dg.DataGenerator(spark, name=\"sensor_readings\", \n",
    "                        rows=int(500000 * 8640 * days * sample_rate),  # Adjust based on sample_rate\n",
    "                        partitions=64)\n",
    "        .withColumn(\"reading_id\", \"string\", expr=\"uuid()\")\n",
    "        .withColumn(\"sensor_idx\", \"int\", minValue=0, maxValue=499999, random=True)  # 500K sensors\n",
    "        .withColumn(\"day_offset\", \"int\", minValue=0, maxValue=days-1, random=True)\n",
    "        .withColumn(\"second_of_day\", \"int\", minValue=0, maxValue=86399, random=True)\n",
    "        .withColumn(\"base_value\", \"double\", minValue=0.3, maxValue=0.7, random=True)  # Normalized\n",
    "        .withColumn(\"noise\", \"double\", minValue=-0.05, maxValue=0.05, random=True)\n",
    "        .withColumn(\"anomaly_flag\", \"double\", expr=\"rand()\")  # For introducing anomalies\n",
    "        .withColumn(\"reading_quality\", \"string\", \n",
    "                   values=[\"GOOD\", \"UNCERTAIN\", \"BAD\"], \n",
    "                   weights=[0.97, 0.025, 0.005], random=True)\n",
    "        .withColumn(\"is_interpolated\", \"boolean\", expr=\"rand() > 0.99\")\n",
    "    )\n",
    "    \n",
    "    df = readings_spec.build()\n",
    "    \n",
    "    # Join with sensor metadata to get proper ranges\n",
    "    df = df.join(\n",
    "        F.broadcast(sensor_data.withColumn(\"sensor_idx_join\", \n",
    "            F.expr(\"cast(substring(sensor_id, 6) as int)\"))),\n",
    "        df.sensor_idx == F.col(\"sensor_idx_join\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    # Calculate actual reading value based on sensor's valid range\n",
    "    df = df.withColumn(\n",
    "        \"reading_value\",\n",
    "        F.when(F.col(\"anomaly_flag\") > 0.995,  # 0.5% anomalies\n",
    "               F.col(\"min_value\") + (F.col(\"base_value\") + F.col(\"noise\") + 0.3) * (F.col(\"max_value\") - F.col(\"min_value\")))\n",
    "        .otherwise(\n",
    "            F.col(\"min_value\") + (F.col(\"base_value\") + F.col(\"noise\")) * (F.col(\"max_value\") - F.col(\"min_value\"))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Calculate timestamps\n",
    "    start_timestamp = int(datetime.combine(start_date, datetime.min.time()).timestamp())\n",
    "    df = df.withColumn(\n",
    "        \"reading_timestamp\",\n",
    "        F.from_unixtime(F.lit(start_timestamp) + F.col(\"day_offset\") * 86400 + F.col(\"second_of_day\"))\n",
    "    ).withColumn(\n",
    "        \"event_time\", F.col(\"reading_timestamp\")\n",
    "    ).withColumn(\n",
    "        \"processing_time\", \n",
    "        F.from_unixtime(F.unix_timestamp(F.col(\"reading_timestamp\")) + F.lit(1))\n",
    "    )\n",
    "    \n",
    "    # Select final columns\n",
    "    return df.select(\n",
    "        \"reading_id\", \"sensor_id\", \"unit_id\", \"refinery_id\",\n",
    "        \"reading_timestamp\", \"reading_value\", \"reading_quality\",\n",
    "        \"is_interpolated\", \"event_time\", \"processing_time\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c894d350-6b84-44c5-8289-53a900adc92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- fact_crude_shipments ---\n",
    "def generate_crude_shipments(spark, refineries_df, crude_types_df, tanks_df, num_shipments=10000000):\n",
    "    refinery_ids = [row.refinery_id for row in refineries_df.select(\"refinery_id\").collect()]\n",
    "    crude_ids = [row.crude_type_id for row in crude_types_df.select(\"crude_type_id\").collect()]\n",
    "    \n",
    "    transport_modes = [\"Tanker\", \"Pipeline\", \"Barge\", \"Rail\"]\n",
    "    ports = [\"Ras Tanura\", \"Houston\", \"Rotterdam\", \"Singapore\", \"Fujairah\", \n",
    "             \"Corpus Christi\", \"Louisiana Offshore\", \"Bonny\", \"Primorsk\"]\n",
    "    statuses = [\"SCHEDULED\", \"IN_TRANSIT\", \"DELIVERED\", \"DELAYED\"]\n",
    "    suppliers = [f\"SUPP-{str(i).zfill(3)}\" for i in range(1, 51)]\n",
    "    vessels = [f\"MT {name}\" for name in [\"Pacific Star\", \"Atlantic Grace\", \"Gulf Spirit\", \n",
    "               \"Ocean Pride\", \"Coastal Dawn\", \"Marine King\", \"Sea Fortune\", \"Tanker Alpha\"]]\n",
    "    \n",
    "    shipment_spec = (\n",
    "        dg.DataGenerator(spark, name=\"crude_shipments\", rows=num_shipments, partitions=32)\n",
    "        .withColumn(\"shipment_id\", \"string\", expr=\"concat('SHIP-', lpad(cast(id as string), 7, '0'))\")\n",
    "        .withColumn(\"refinery_id\", \"string\", values=refinery_ids, random=True)\n",
    "        .withColumn(\"crude_type_id\", \"string\", values=crude_ids, random=True)\n",
    "        .withColumn(\"supplier_id\", \"string\", values=suppliers, random=True)\n",
    "        .withColumn(\"vessel_name\", \"string\", values=vessels, random=True)\n",
    "        .withColumn(\"transport_mode\", \"string\", values=transport_modes, \n",
    "                   weights=[0.5, 0.3, 0.15, 0.05], random=True)\n",
    "        .withColumn(\"origin_port\", \"string\", values=ports, random=True)\n",
    "        .withColumn(\"shipment_date\", \"date\", begin=\"2020-01-01\", end=\"2024-12-31\", random=True)\n",
    "        .withColumn(\"transit_days\", \"int\", minValue=5, maxValue=45, random=True)\n",
    "        .withColumn(\"volume_barrels\", \"int\", minValue=100000, maxValue=2500000, step=50000, random=True)\n",
    "        .withColumn(\"api_gravity_actual\", \"double\", minValue=20.0, maxValue=50.0, random=True)\n",
    "        .withColumn(\"sulfur_content_actual\", \"double\", minValue=0.05, maxValue=4.0, random=True)\n",
    "        .withColumn(\"water_content_pct\", \"double\", minValue=0.0, maxValue=1.0, random=True)\n",
    "        .withColumn(\"price_per_barrel\", \"double\", minValue=40.0, maxValue=120.0, random=True)\n",
    "        .withColumn(\"status\", \"string\", values=statuses, weights=[0.05, 0.1, 0.8, 0.05], random=True)\n",
    "    )\n",
    "    \n",
    "    df = shipment_spec.build()\n",
    "    \n",
    "    # Calculate derived columns\n",
    "    df = df.withColumn(\"arrival_date\", F.date_add(\"shipment_date\", F.col(\"transit_days\")))\n",
    "    df = df.withColumn(\"expected_arrival_date\", \n",
    "                       F.date_add(\"shipment_date\", F.col(\"transit_days\") - F.floor(F.rand() * 3).cast(\"int\")))\n",
    "    df = df.withColumn(\"total_cost\", F.col(\"volume_barrels\") * F.col(\"price_per_barrel\"))\n",
    "    df = df.withColumn(\"created_at\", F.to_timestamp(\"shipment_date\"))\n",
    "    df = df.withColumn(\"updated_at\", F.to_timestamp(\"arrival_date\"))\n",
    "    \n",
    "    # Add destination tank (join with tanks)\n",
    "    tank_ids = [row.tank_id for row in tanks_df.filter(F.col(\"product_type\") == \"Crude Oil\")\n",
    "                .select(\"tank_id\").collect()]\n",
    "    df = df.withColumn(\"destination_tank_id\", \n",
    "                       F.element_at(F.array([F.lit(t) for t in tank_ids[:100]]), \n",
    "                                   (F.floor(F.rand() * len(tank_ids[:100])) + 1).cast(\"int\")))\n",
    "    \n",
    "    return df.drop(\"transit_days\", \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d982335-9567-4203-a5e2-ce2bd8daa92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- fact_quality_tests ---\n",
    "def generate_quality_tests(spark, refineries_df, shipments_df, tanks_df, num_tests=15000000):\n",
    "    refinery_ids = [row.refinery_id for row in refineries_df.select(\"refinery_id\").collect()]\n",
    "    \n",
    "    test_types = [\"Full Assay\", \"Spot Check\", \"Blend Verification\", \"Product Certification\"]\n",
    "    source_types = [\"Crude Shipment\", \"Tank Sample\", \"Process Stream\", \"Product Output\"]\n",
    "    statuses = [\"PASS\", \"FAIL\", \"MARGINAL\", \"PENDING\"]\n",
    "    technicians = [f\"TECH-{str(i).zfill(3)}\" for i in range(1, 101)]\n",
    "    \n",
    "    test_spec = (\n",
    "        dg.DataGenerator(spark, name=\"quality_tests\", rows=num_tests, partitions=32)\n",
    "        .withColumn(\"test_id\", \"string\", expr=\"concat('TEST-', lpad(cast(id as string), 8, '0'))\")\n",
    "        .withColumn(\"refinery_id\", \"string\", values=refinery_ids, random=True)\n",
    "        .withColumn(\"sample_source_type\", \"string\", values=source_types, \n",
    "                   weights=[0.3, 0.4, 0.2, 0.1], random=True)\n",
    "        .withColumn(\"test_datetime\", \"timestamp\", begin=\"2020-01-01 00:00:00\", end=\"2024-12-31 00:00:00\", random=True)\n",
    "        .withColumn(\"test_type\", \"string\", values=test_types, \n",
    "                   weights=[0.2, 0.5, 0.2, 0.1], random=True)\n",
    "        .withColumn(\"technician_id\", \"string\", values=technicians, random=True)\n",
    "        .withColumn(\"api_gravity\", \"double\", minValue=18.0, maxValue=55.0, random=True)\n",
    "        .withColumn(\"sulfur_content_pct\", \"double\", minValue=0.01, maxValue=5.0, random=True)\n",
    "        .withColumn(\"water_sediment_pct\", \"double\", minValue=0.0, maxValue=2.0, random=True)\n",
    "        .withColumn(\"viscosity_cst\", \"double\", minValue=1.0, maxValue=500.0, random=True)\n",
    "        .withColumn(\"pour_point_f\", \"double\", minValue=-40.0, maxValue=60.0, random=True)\n",
    "        .withColumn(\"reid_vapor_pressure\", \"double\", minValue=5.0, maxValue=15.0, random=True)\n",
    "        .withColumn(\"flash_point_f\", \"double\", minValue=100.0, maxValue=250.0, random=True)\n",
    "        .withColumn(\"test_result_status\", \"string\", values=statuses, \n",
    "                   weights=[0.85, 0.05, 0.08, 0.02], random=True)\n",
    "        .withColumn(\"notes\", \"string\", values=[\"Within spec\", \"Minor variance\", \"Requires retest\", \n",
    "                                               \"Approved\", \"See lab report\"], random=True)\n",
    "    )\n",
    "    \n",
    "    df = test_spec.build()\n",
    "    \n",
    "    # Add nullable fields\n",
    "    df = df.withColumn(\"octane_number\", \n",
    "                       F.when(F.rand() > 0.7, F.lit(None)).otherwise(F.rand() * 10 + 85))\n",
    "    df = df.withColumn(\"cetane_number\",\n",
    "                       F.when(F.rand() > 0.7, F.lit(None)).otherwise(F.rand() * 15 + 40))\n",
    "    df = df.withColumn(\"sample_source_id\", F.expr(\"concat('SRC-', lpad(cast(floor(rand()*1000000) as string), 7, '0'))\"))\n",
    "    df = df.withColumn(\"tank_id\", \n",
    "                       F.when(F.col(\"sample_source_type\") == \"Tank Sample\",\n",
    "                             F.expr(\"concat('TANK-', lpad(cast(floor(rand()*2000) as string), 5, '0'))\"))\n",
    "                       .otherwise(F.lit(None)))\n",
    "    \n",
    "    return df.drop(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84ae90ea-c718-489c-a7c7-17a19efe4f11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- fact_tank_inventory ---\n",
    "def generate_tank_inventory(spark, tanks_df, days_of_data=365):\n",
    "    tank_ids = [row.tank_id for row in tanks_df.select(\"tank_id\").collect()]\n",
    "    tank_refineries = {row.tank_id: row.refinery_id \n",
    "                      for row in tanks_df.select(\"tank_id\", \"refinery_id\").collect()}\n",
    "    tank_products = {row.tank_id: row.product_type \n",
    "                    for row in tanks_df.select(\"tank_id\", \"product_type\").collect()}\n",
    "    tank_capacities = {row.tank_id: row.capacity_barrels \n",
    "                      for row in tanks_df.select(\"tank_id\", \"capacity_barrels\").collect()}\n",
    "    \n",
    "    # 96 readings per day (every 15 min) × days × tanks\n",
    "    readings_per_day = 96\n",
    "    total_readings = len(tank_ids) * readings_per_day * days_of_data\n",
    "    \n",
    "    inv_spec = (\n",
    "        dg.DataGenerator(spark, name=\"tank_inventory\", rows=total_readings, partitions=32)\n",
    "        .withColumn(\"inventory_id\", \"string\", expr=\"uuid()\")\n",
    "        .withColumn(\"tank_idx\", \"int\", minValue=0, maxValue=len(tank_ids)-1, random=True)\n",
    "        .withColumn(\"tank_id\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple(tank_ids)}, tank_idx+1)\")\n",
    "        .withColumn(\"day_offset\", \"int\", minValue=0, maxValue=days_of_data-1, random=True)\n",
    "        .withColumn(\"time_slot\", \"int\", minValue=0, maxValue=95, random=True)  # 0-95 for 15-min slots\n",
    "        .withColumn(\"fill_percentage\", \"double\", minValue=0.15, maxValue=0.92, random=True)\n",
    "        .withColumn(\"temperature_f\", \"double\", minValue=60.0, maxValue=120.0, random=True)\n",
    "        .withColumn(\"water_bottom_inches\", \"double\", minValue=0.0, maxValue=12.0, random=True)\n",
    "        .withColumn(\"measurement_method\", \"string\", \n",
    "                   values=[\"Radar Gauge\", \"Manual Dip\", \"Float Gauge\", \"Servo Gauge\"],\n",
    "                   weights=[0.6, 0.1, 0.15, 0.15], random=True)\n",
    "        .withColumn(\"is_verified\", \"boolean\", expr=\"rand() > 0.95\")\n",
    "    )\n",
    "    \n",
    "    df = inv_spec.build()\n",
    "    \n",
    "    # Calculate timestamp\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    df = df.withColumn(\n",
    "        \"measurement_timestamp\",\n",
    "        F.from_unixtime(F.lit(start_ts) + F.col(\"day_offset\") * 86400 + F.col(\"time_slot\") * 900)\n",
    "    )\n",
    "    \n",
    "    # Add refinery_id and product info via lookup (simplified)\n",
    "    # In real implementation, join with tanks_df\n",
    "    df = df.join(F.broadcast(tanks_df.select(\"tank_id\", \"refinery_id\", \"product_type\", \"capacity_barrels\")), \n",
    "                \"tank_id\", \"left\")\n",
    "    \n",
    "    df = df.withColumn(\"volume_barrels\", \n",
    "                       (F.col(\"fill_percentage\") * F.col(\"capacity_barrels\")).cast(\"int\"))\n",
    "    \n",
    "    # Add crude_type_id and product_id based on product_type\n",
    "    df = df.withColumn(\"crude_type_id\",\n",
    "                       F.when(F.col(\"product_type\") == \"Crude Oil\",\n",
    "                             F.expr(\"concat('CRUDE-', lpad(cast(floor(rand()*15)+1 as string), 3, '0'))\"))\n",
    "                       .otherwise(F.lit(None)))\n",
    "    df = df.withColumn(\"product_id\",\n",
    "                       F.when(F.col(\"product_type\") != \"Crude Oil\",\n",
    "                             F.expr(\"concat('PROD-', lpad(cast(floor(rand()*14)+1 as string), 3, '0'))\"))\n",
    "                       .otherwise(F.lit(None)))\n",
    "    \n",
    "    return df.select(\n",
    "        \"inventory_id\", \"tank_id\", \"refinery_id\", \"measurement_timestamp\",\n",
    "        \"volume_barrels\", \"fill_percentage\", \"temperature_f\", \"water_bottom_inches\",\n",
    "        \"product_type\", \"crude_type_id\", \"product_id\", \"measurement_method\", \"is_verified\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9590bc35-28e5-41ff-9942-90640c303fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- fact_production_output ---\n",
    "def generate_production_output(spark, units_df, products_df, crude_types_df, days_of_data=365):\n",
    "    unit_ids = [row.unit_id for row in units_df.select(\"unit_id\").collect()]\n",
    "    product_ids = [row.product_id for row in products_df.select(\"product_id\").collect()]\n",
    "    crude_ids = [row.crude_type_id for row in crude_types_df.select(\"crude_type_id\").collect()]\n",
    "    \n",
    "    # 24 hours × days × units\n",
    "    hours_of_data = 24 * days_of_data\n",
    "    total_records = len(unit_ids) * hours_of_data // 10  # Sample 10%\n",
    "    \n",
    "    output_spec = (\n",
    "        dg.DataGenerator(spark, name=\"production_output\", rows=total_records, partitions=32)\n",
    "        .withColumn(\"output_id\", \"string\", expr=\"uuid()\")\n",
    "        .withColumn(\"unit_idx\", \"int\", minValue=0, maxValue=len(unit_ids)-1, random=True)\n",
    "        .withColumn(\"unit_id\", \"string\",\n",
    "                   expr=f\"element_at(array{tuple(unit_ids[:1000])}, (unit_idx % 1000) + 1)\")  # Limit for expr\n",
    "        .withColumn(\"day_offset\", \"int\", minValue=0, maxValue=days_of_data-1, random=True)\n",
    "        .withColumn(\"hour\", \"int\", minValue=0, maxValue=23, random=True)\n",
    "        .withColumn(\"input_volume_barrels\", \"int\", minValue=3000, maxValue=10000, step=100, random=True)\n",
    "        .withColumn(\"input_crude_type_id\", \"string\", values=crude_ids, random=True)\n",
    "        .withColumn(\"output_product_id\", \"string\", values=product_ids, random=True)\n",
    "        .withColumn(\"yield_percentage\", \"double\", minValue=0.75, maxValue=0.98, random=True)\n",
    "        .withColumn(\"energy_consumed_mmbtu\", \"double\", minValue=500.0, maxValue=2500.0, random=True)\n",
    "        .withColumn(\"operating_temperature_f\", \"double\", minValue=400.0, maxValue=900.0, random=True)\n",
    "        .withColumn(\"operating_pressure_psi\", \"double\", minValue=20.0, maxValue=100.0, random=True)\n",
    "        .withColumn(\"downtime_minutes\", \"int\", values=[0, 0, 0, 0, 0, 15, 30, 60], random=True)\n",
    "        .withColumn(\"quality_grade\", \"string\", \n",
    "                   values=[\"On-Spec\", \"Off-Spec\", \"Premium\"], \n",
    "                   weights=[0.85, 0.10, 0.05], random=True)\n",
    "        .withColumn(\"shift_id\", \"string\", values=[\"SHIFT-A\", \"SHIFT-B\", \"SHIFT-C\"], random=True)\n",
    "    )\n",
    "    \n",
    "    df = output_spec.build()\n",
    "    \n",
    "    # Calculate production hour timestamp\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    start_ts = int(start_date.timestamp())\n",
    "    df = df.withColumn(\n",
    "        \"production_hour\",\n",
    "        F.from_unixtime(F.lit(start_ts) + F.col(\"day_offset\") * 86400 + F.col(\"hour\") * 3600)\n",
    "    )\n",
    "    \n",
    "    # Calculate output volume\n",
    "    df = df.withColumn(\"output_volume_barrels\",\n",
    "                       (F.col(\"input_volume_barrels\") * F.col(\"yield_percentage\")).cast(\"int\"))\n",
    "    \n",
    "    # Add refinery_id via join\n",
    "    df = df.join(F.broadcast(units_df.select(\"unit_id\", \"refinery_id\")), \"unit_id\", \"left\")\n",
    "    \n",
    "    return df.select(\n",
    "        \"output_id\", \"unit_id\", \"refinery_id\", \"production_hour\",\n",
    "        \"input_volume_barrels\", \"input_crude_type_id\", \"output_product_id\",\n",
    "        \"output_volume_barrels\", \"yield_percentage\", \"energy_consumed_mmbtu\",\n",
    "        \"operating_temperature_f\", \"operating_pressure_psi\", \"downtime_minutes\",\n",
    "        \"quality_grade\", \"shift_id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80fff33a-3f5d-4922-8fdc-33d3c5d190e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4138acf2-38ef-47d7-b7c4-56d198cf0f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29297960-0eeb-457b-b59c-57f995e320c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_all_data(spark, catalog, schema, sample_rate=0.1):\n",
    "    \"\"\"\n",
    "    Main function to generate all datasets.\n",
    "    sample_rate: Controls data volume (0.1 = ~10GB, 1.0 = ~100GB)\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    from datetime import date\n",
    "    spark.sql(f\"create schema if not exists {catalog}.{schema}\")\n",
    "    spark.sql(f\"use {catalog}.{schema}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CRUDE OIL REFINERY DATA GENERATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Generate dimension tables\n",
    "    print(\"\\n[1/11] Generating dim_refineries...\")\n",
    "    refineries = generate_refineries(spark, 500)\n",
    "    refineries.write.mode(\"overwrite\").saveAsTable(f\"dim_refineries\")\n",
    "    print(f\"       Generated {refineries.count()} refineries\")\n",
    "    \n",
    "    print(\"\\n[2/11] Generating dim_crude_types...\")\n",
    "    crude_types = generate_crude_types(spark)\n",
    "    crude_types.write.mode(\"overwrite\").saveAsTable(f\"dim_crude_types\")\n",
    "    print(f\"       Generated {crude_types.count()} crude types\")\n",
    "    \n",
    "    print(\"\\n[3/11] Generating dim_products...\")\n",
    "    products = generate_products(spark)\n",
    "    products.write.mode(\"overwrite\").saveAsTable(f\"dim_products\")\n",
    "    print(f\"       Generated {products.count()} products\")\n",
    "    \n",
    "    print(\"\\n[4/11] Generating dim_processing_units...\")\n",
    "    units = generate_processing_units(spark, refineries, 10)\n",
    "    units.write.mode(\"overwrite\").saveAsTable(f\"dim_processing_units\")\n",
    "    print(f\"       Generated {units.count()} processing units\")\n",
    "    \n",
    "    print(\"\\n[5/11] Generating dim_storage_tanks...\")\n",
    "    tanks = generate_storage_tanks(spark, refineries, 4)\n",
    "    tanks.write.mode(\"overwrite\").saveAsTable(f\"dim_storage_tanks\")\n",
    "    print(f\"       Generated {tanks.count()} tanks\")\n",
    "    \n",
    "    print(\"\\n[6/11] Generating dim_sensors...\")\n",
    "    sensors = generate_sensors(spark, units, 100)\n",
    "    sensors.write.mode(\"overwrite\").saveAsTable(f\"dim_sensors\")\n",
    "    print(f\"       Generated {sensors.count()} sensors\")\n",
    "    \n",
    "    # 2. Generate fact tables\n",
    "    print(\"\\n[7/11] Generating fact_crude_shipments...\")\n",
    "    shipments = generate_crude_shipments(spark, refineries, crude_types, tanks, \n",
    "                                        int(10000000 * sample_rate))\n",
    "    shipments.write.mode(\"overwrite\").saveAsTable(f\"fact_crude_shipments\")\n",
    "    print(f\"       Generated {shipments.count()} shipments\")\n",
    "    \n",
    "    print(\"\\n[8/11] Generating fact_quality_tests...\")\n",
    "    quality_tests = generate_quality_tests(spark, refineries, shipments, tanks,\n",
    "                                          int(15000000 * sample_rate))\n",
    "    quality_tests.write.mode(\"overwrite\").saveAsTable(f\"fact_quality_tests\")\n",
    "    print(f\"       Generated {quality_tests.count()} quality tests\")\n",
    "    \n",
    "    print(\"\\n[9/11] Generating fact_tank_inventory...\")\n",
    "    tank_inventory = generate_tank_inventory(spark, tanks, 365)\n",
    "    tank_inventory.write.mode(\"overwrite\").saveAsTable(f\"fact_tank_inventory\")\n",
    "    print(f\"       Generated {tank_inventory.count()} inventory records\")\n",
    "    \n",
    "    print(\"\\n[10/11] Generating fact_production_output...\")\n",
    "    production = generate_production_output(spark, units, products, crude_types, 365)\n",
    "    production.write.mode(\"overwrite\").saveAsTable(f\"fact_production_output\")\n",
    "    print(f\"       Generated {production.count()} production records\")\n",
    "    \n",
    "    print(\"\\n[11/11] Generating fact_sensor_readings (batch - historical)...\")\n",
    "    start_date = date(2024, 1, 1)\n",
    "    end_date = date(2024, 12, 31)\n",
    "    sensor_readings = generate_sensor_readings_batch(spark, sensors, start_date, end_date, \n",
    "                                                    sample_rate=sample_rate * 0.1)\n",
    "    sensor_readings.write.mode(\"overwrite\").saveAsTable(f\"fact_sensor_readings\")\n",
    "    print(f\"       Generated {sensor_readings.count()} sensor readings\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA GENERATION COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nData Location:\", f\"{catalog}.{schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce6a644-f7b0-4600-b092-af7cb6551685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run generation\n",
    "generate_all_data(spark, CATALOG, SCHEMA, sample_rate=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_generate_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}